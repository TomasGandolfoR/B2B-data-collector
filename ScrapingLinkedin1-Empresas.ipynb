{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nro de archivo a crear\n",
    "nro_de_archivo = 3\n",
    "\n",
    "# existente\n",
    "Path_linkedin_empresas = 'linkedin_empresas_mkt_11a50empleados.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path_linkedin_empresas)\n",
    "df = df['Column_Name']\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buscar un divisor redondo para el df\n",
    "div_df = int(len(df)/5)\n",
    "div_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df[0:div_df]\n",
    "df_2 = df[div_df:div_df*2]\n",
    "df_3 = df[div_df*2:div_df*3]\n",
    "df_4 = df[div_df*3:div_df*4]\n",
    "df_5 = df[div_df*4:div_df*5]\n",
    "\n",
    "dataframes = [df_1, df_2, df_3, df_4, df_5]\n",
    "\n",
    "print(len(df_1)+len(df_2)+len(df_3)+len(df_4)+len(df_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buscar en google a las personas de cada empresa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml.html import fromstring\n",
    "import urllib.request\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "np.random.seed(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetFirstLinkedinPersonProfile (soup):\n",
    "    aux = ''\n",
    "    for link in soup.find_all('a'):\n",
    "        if str(link).find(\"linkedin.com/in/\") != -1 and str(link).find(\"https://\") != -1 and str(link).find(\"google\") == -1:\n",
    "            aux = link.get('href')[link.get('href').find('https://'):link.get('href').find('&sa')]\n",
    "            print(aux)\n",
    "            return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml.html import fromstring\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            #Grabbing IP and corresponding PORT\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return cycle(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback\n",
    "def GetPoolProxisFromProxisCsvOrDoGet_proxies ():\n",
    "    try:\n",
    "        proxies_df = pd.read_csv('proxies.csv', names = [\"proxy\"])\n",
    "        proxy_pool = cycle(proxies_df[\"proxy\"])\n",
    "        return proxy_pool\n",
    "    except:\n",
    "        proxies = get_proxies()\n",
    "        proxy_pool = cycle(proxies)\n",
    "        return proxy_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nro_random(multiplicador, piso = 0.01):\n",
    "    tiempo = 0\n",
    "    while tiempo < piso:\n",
    "        tiempo = np.random.random() * multiplicador\n",
    "    return tiempo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_aux = dataframes[nro_de_archivo-1]\n",
    "data_1 = []\n",
    "\n",
    "a = 'https://www.google.com.ar/search?source=hp&ei=7oxuX7uUL_m95OUPmOWfiAg&q=site%3Alinkedin.com%2Fin%2F+AND+%28founder+OR+CEO+OR+fundador%29+AND+%22'\n",
    "b = '%22&oq=site%3Alinkedin.com%2Fin%2F+AND+%28founder+OR+CEO+OR+fundador%29+AND+%22bakian-growthmarketing%22&gs_lcp=CgZwc3ktYWIQA1DzD1iPE2DkFmgAcAB4AoABAIgBAJIBAJgBBaABAaoBB2d3cy13aXo&sclient=psy-ab&ved=0ahUKEwi7w9nvyYXsAhX5HrkGHZjyB4EQ4dUDCAc&uact=5'\n",
    "indice = len('https://www.linkedin.com/company/')\n",
    "\n",
    "\n",
    "#proxy_pool = GetPoolProxisFromProxisCsvOrDoGet_proxies()\n",
    "\n",
    "#i = 0\n",
    "#proxy_ok = True\n",
    "#proxy = next(proxy_pool)\n",
    "\n",
    "for item in df_aux:\n",
    "    #\n",
    "    '''if proxy_ok != True:\n",
    "        proxy = next(proxy_pool)   \n",
    "        \n",
    "    print(\"Request #%d\"%i)'''\n",
    "    #\n",
    "    #item = df_aux[i]\n",
    "    url = a + item[indice:] + b\n",
    "    \n",
    "    '''try:\n",
    "        proxyDict = { \n",
    "              \"http\"  : \"http://\" + proxy, \n",
    "              \"https\" : \"https://\" + proxy, \n",
    "              \"ftp\"   : \"ftp://\" + proxy\n",
    "            }'''\n",
    "    #html = requests.get(url, proxies = proxyDict)\n",
    "    html = requests.get(url)\n",
    "    '''except:\n",
    "        proxy_ok = False\n",
    "        continue'''\n",
    "    html_doc = str(html.content)\n",
    "    \n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "    link = GetFirstLinkedinPersonProfile(soup)\n",
    "    \n",
    "    #if link != \"None\":\n",
    "    data_1.append(link)\n",
    "\n",
    "    time.sleep(nro_random(13, 3))\n",
    "    '''else:\n",
    "        proxy_ok = False'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(data_1, columns = [\"link_contacto\"])\n",
    "dataframe[\"link_empresa\"] = dataframes[nro_de_archivo-1].reset_index(drop=True)\n",
    "dataframe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"data_N_ok/data_\"+str(nro_de_archivo) +\".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
